{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "outdoor-start",
   "metadata": {},
   "source": [
    "#### Arcpy + Geopandas combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "respected-raleigh",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Must be opened from ArcGIS Python Command Prompt (juptyter lab)\n",
    "# Must have signed into ArcGIS online? (run ArcGIS Pro)\n",
    "\n",
    "import arcpy\n",
    "from arcpy import env\n",
    "from arcpy.sa import *\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "interested-investor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def renameShapeField(inshp,infieldname,outfieldname):\n",
    "    '''renames a field in a shapefile using geopandas because arcpy doesn't support this.'''\n",
    "    ingdf = gpd.read_file(inshp)\n",
    "    ingdf.rename(columns={infieldname: outfieldname}, inplace=True)\n",
    "    ingdf.to_file(inshp)\n",
    "    print(f'Fieldname {infieldname} changed to {outfieldname} in shapefile.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "destroyed-lithuania",
   "metadata": {},
   "outputs": [],
   "source": [
    "##inputs\n",
    "#======================= Brandy 2019-2018\n",
    "#output dir\n",
    "demdiff_dir = Path(r\"D:\\Whiskeytown\\dem_diff\\brandy_creek\\demdiff19-18\")\n",
    "#DEMS\n",
    "dem1 = Path(r\"D:\\Whiskeytown\\dem_diff\\brandy_creek\\dems\\orig\\brandy_dem2018.tif\")\n",
    "dem2 = Path(r\"D:\\Whiskeytown\\dem_diff\\brandy_creek\\dems\\orig\\brandy_dem2019.tif\")\n",
    "#Stable polygons\n",
    "stablepolyshp = Path(r\"D:\\Whiskeytown\\dem_diff\\brandy_creek\\shp\\Stable_poly_18-19_expanded.shp\")\n",
    "#Output dod name stem\n",
    "dodnamestem = r\"DoD_19-18\"\n",
    "detrendnamestem = r\"Detrend_19-18_polyn\"\n",
    "#region\n",
    "aoi = 'brandy'\n",
    "\n",
    "# #======================= Brandy 2020-2019\n",
    "# #output dir\n",
    "# demdiff_dir = Path(r\"D:\\Whiskeytown\\dem_diff\\brandy_creek\\demdiff20-19\")\n",
    "# #DEMS\n",
    "# dem1 = Path(r\"D:\\Whiskeytown\\dem_diff\\brandy_creek\\dems\\orig\\brandy_dem2019.tif\")\n",
    "# dem2 = Path(r\"D:\\Whiskeytown\\dem_diff\\brandy_creek\\dems\\orig\\brandy_dem2020.tif\")\n",
    "# #Stable polygons\n",
    "# stablepolyshp = Path(r\"D:\\Whiskeytown\\dem_diff\\brandy_creek\\shp\\Stable_poly_19-20_expanded.shp\")\n",
    "# #Output dod name stem\n",
    "# dodnamestem = r\"DoD_20-19\"\n",
    "# detrendnamestem = r\"Detrend_20-19_polyn\"\n",
    "# #region\n",
    "# aoi = 'brandy'\n",
    "\n",
    "# #======================= Boulder 2019-2018\n",
    "# #output dir\n",
    "# demdiff_dir = Path(r\"D:\\Whiskeytown\\dem_diff\\boulder_creek\\demdiff19-18\")\n",
    "# #DEMS\n",
    "# dem1 = Path(r\"D:\\Whiskeytown\\dem_diff\\boulder_creek\\dem\\original\\boulder_dem2018.tif\")\n",
    "# dem2 = Path(r\"D:\\Whiskeytown\\dem_diff\\boulder_creek\\dem\\original\\boulder_dem2019.tif\")\n",
    "# #Stable polygons\n",
    "# stablepolyshp = Path(r\"D:\\Whiskeytown\\dem_diff\\boulder_creek\\shp\\Stable_poly_18-19.shp\")\n",
    "# #Output dod name stem\n",
    "# dodnamestem = r\"DoD_19-18\"\n",
    "# detrendnamestem = r\"Detrend_19-18_polyn\"\n",
    "# #region\n",
    "# aoi = 'boulder'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "contemporary-academy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set standard output dir and create dir\n",
    "outdod_dir = Path(demdiff_dir, r\"dod\")\n",
    "outdod_unadj_dir  = Path(demdiff_dir, r\"dod\\unadj\")\n",
    "outdod_adj_dir  = Path(demdiff_dir, r\"dod\\adj\")\n",
    "outdod_pt_dir  = Path(demdiff_dir, r\"dod\\shp\")\n",
    "outtrendraster_dir = Path(demdiff_dir, r\"detrend\")\n",
    "outtrend_pt_dir = Path(demdiff_dir, r\"detrend\\shp\")\n",
    "outcoregdem_dir = Path(demdiff_dir, r\"co-reg-dem\")\n",
    "scratch_dir = Path(demdiff_dir, r\"arcpyscratch\")\n",
    "\n",
    "#create parent if doesn't exist\n",
    "demdiff_dir.mkdir(parents=True, exist_ok=True)\n",
    "#create subdir if don't exist\n",
    "for direc in [outdod_dir, outdod_unadj_dir, outdod_adj_dir, outdod_pt_dir, outtrendraster_dir, outtrend_pt_dir, outcoregdem_dir, scratch_dir]:\n",
    "    direc.mkdir(parents=True, exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "received-universal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment settings\n",
    "arcpy.env.workspace = str(scratch_dir)\n",
    "arcpy.env.overwriteOutput = True\n",
    "arcpy.env.compression = \"LZW\"\n",
    "\n",
    "env.extent = str(dem1)\n",
    "env.snapRaster = str(dem1)\n",
    "\n",
    "# Check out the ArcGIS Spatial Analyst extension license\n",
    "arcpy.CheckOutExtension(\"Spatial\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honey-cookbook",
   "metadata": {},
   "source": [
    "### 1. Make Unadjusted DoD \n",
    "(DEM2 - DEM1 = Unadjusted_DoD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "guided-midnight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating unadjusted DoD ...\n"
     ]
    }
   ],
   "source": [
    "#Make unadjusted DoD\n",
    "print('Creating unadjusted DoD ...')\n",
    "outunadjdod = Path(outdod_unadj_dir,dodnamestem + '_' + aoi + r'_unadj.tif')\n",
    "\n",
    "dod = RasterCalculator([str(dem1), str(dem2)], [\"dem1\", \"dem2\"],\n",
    "                                       \"dem2-dem1\", \"FirstOf\", \"FirstOf\")\n",
    "\n",
    "dod.save(str(outunadjdod))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nutritional-tunnel",
   "metadata": {},
   "source": [
    "### 2. Create point shapefile of unadjusted DoD, then clip with stable area polygons \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "stylish-essay",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverting DoD to points ...\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'XTools'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\ESRI\\conda\\envs\\arcgispro-py3-jlogan\\lib\\site-packages\\XTools Pro\\esri\\toolboxes\\XTools Pro.pyt\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32mC:\\ArcGIS\\XToolsAGP\\Python\\Tools\\FeaturesToPointsTool.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mXTools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mXToolsAGP\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGeoprocessing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mXTools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mXToolsAGP\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGeoprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFeaturesToPointsGpTool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFeaturesToPointsGp_EqPointsNumberTool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFeaturesToPoints_EqPointsIntervalGpTool\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mScripts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonCancelTracker\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPythonCancelTracker\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'XTools'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clipping points to stable areas ...\n",
      "Spatial join to get polygon type attribute to points ...\n",
      "Renaming field in point shapefile ...\n",
      "Fieldname grid_code changed to dod_unadj in shapefile.\n"
     ]
    }
   ],
   "source": [
    "#Convert DoD to point shapefile, then clip with stable polygons and rename field to 'dod_unadj'\n",
    "#inputs\n",
    "outstablept_shp = str(Path(outdod_pt_dir, outunadjdod.stem + r'_stable_pts.shp')) #same as DoD, but '_stable_pts.shp')\n",
    "\n",
    "#covert to points scratchfile\n",
    "print('Coverting DoD to points ...')\n",
    "arcpy.RasterToPoint_conversion(dod, r\"memory\\tempRasPt\", \"VALUE\");\n",
    "#clip scratchfile\n",
    "print('Clipping points to stable areas ...')\n",
    "arcpy.Clip_analysis(r\"memory\\tempRasPt\", str(stablepolyshp), r\"memory\\tempRasPtClip\", \"\");\n",
    "#get polygon \"type (paved, unpaved)\" onto att table via spatial join\n",
    "print('Spatial join to get polygon type attribute to points ...')\n",
    "arcpy.analysis.SpatialJoin(r\"memory\\tempRasPtClip\", str(stablepolyshp), outstablept_shp);\n",
    "\n",
    "#rename field\n",
    "print('Renaming field in point shapefile ...')\n",
    "renameShapeField(outstablept_shp,'grid_code','dod_unadj')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efficient-voluntary",
   "metadata": {},
   "source": [
    "#### 3. Calculate Error Trend Surfaces using residual values in stable area polygons (which should be zero) \n",
    "- Polynomial order 0 = uniform correction (mean of residuals)\n",
    "- Polynomial order 1 = sloped plane\n",
    "- Polynomial order 2 = quadratic surface\n",
    "- Polynomial order 3 = cubic surface\n",
    "- Polynomial order 4 = quartic surface\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "collected-ratio",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating trend surface polynomial order 0...\n",
      "Creating trend surface polynomial order 1...\n",
      "Creating trend surface polynomial order 2...\n",
      "Creating trend surface polynomial order 3...\n",
      "Creating trend surface polynomial order 4...\n"
     ]
    }
   ],
   "source": [
    "#Calculate trend surface using all stable points (polynomial 0, 1 and 2) (and 3, 4?)\n",
    "\n",
    "#Inputs/outputs\n",
    "stablept_shp = outstablept_shp #(from cell above)\n",
    "outtrendrasterstem = str(Path(outtrendraster_dir, detrendnamestem))\n",
    "\n",
    "zField = \"dod_unadj\"\n",
    "cellSize = 0.25\n",
    "# PolynomialOrder = 2 (set in loop)\n",
    "regressionType = \"LINEAR\"\n",
    "\n",
    "for polyn_order in [0, 1, 2, 3, 4]:\n",
    "    # Execute Trend\n",
    "    print(f'Creating trend surface polynomial order {polyn_order}...')\n",
    "    outTrend = Trend(stablept_shp, zField, cellSize, \n",
    "                     polyn_order, regressionType)\n",
    "    outTrend.save(outtrendrasterstem + str(polyn_order) + \".tif\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "center-entity",
   "metadata": {},
   "source": [
    "#### 4. Apply Error Trend surfaces to DoD to create Adjusted DoD for visual comparisons.\n",
    "Adjustment order is:  \n",
    " DoD - TrendRaster = AdjustedDoD\n",
    "   \n",
    "This is equivalent to adjusting DEM1:  \n",
    " DEM2 - DEM1 - TrendRaster = AdjustedDoD\n",
    " \n",
    "Since  \n",
    "TrendRaster = DoD residual which should be zero.   \n",
    "\n",
    "For example:  \n",
    "DEM2 = 3, DEM1 = 5, DoD = -2, TrendRaster = -2  \n",
    "AdjustedDoD =  DEM2 - DEM1 - TrendRaster = 0  \n",
    "             =  3 - 5 -(-2) = 0  \n",
    "             =  DoD - TrendRaster = 0  \n",
    "             =  -2 -(-2) = 0 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ultimate-cemetery",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# Apply to DoD to evaluate visually\n",
    "# Adjustment order is:\n",
    "#     DoD - TrendRaster = AdjustedDoD\n",
    "# This is equivalent to adjusting DEM1:\n",
    "#      DEM2 - DEM1 - TrendRaster = AdjustedDoD\n",
    "# Since TrendRaster = DoD residual which should be zero.\n",
    "# For example:\n",
    "# DEM2 = 3, DEM1 = 5, DoD = -2, TrendRaster = -2\n",
    "# AdjustedDoD =  DEM2 - DEM1 - TrendRaster = 0\n",
    "#             =  3 - 5 -(-2) = 0\n",
    "#             =  DoD - TrendRaster = 0\n",
    "#             =  -2 -(-2) = 0\n",
    "# '''\n",
    "\n",
    "#inputs/outputs\n",
    "inunadjdod = str(outunadjdod) #from above cell\n",
    "intrendrasterstem = outtrendrasterstem #from cell above\n",
    "outadjdodstem = str(Path(outdod_adj_dir, dodnamestem + '_' + aoi + r'_adj_polyn_'))\n",
    "\n",
    "#loop through detrend surfaces and apply to DoD\n",
    "for polyn_order in [0,1,2,3,4]:\n",
    "    #apply adjustment\n",
    "    trendraster = intrendrasterstem + str(polyn_order) + '.tif'\n",
    "    adj = RasterCalculator([inunadjdod, trendraster], [\"dod\", \"trendraster\"],\n",
    "                                       \"dod-trendraster\", \"FirstOf\", \"FirstOf\")\n",
    "    adj.save(outadjdodstem + str(polyn_order) + '.tif')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nervous-hundred",
   "metadata": {},
   "source": [
    "### 5. Sanity check!  Make sure that ErrorTrendSurface is being correctly applied to DoD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "former-microwave",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEM1: D:\\Whiskeytown\\dem_diff\\brandy_creek\\dems\\orig\\brandy_dem2018.tif\n",
      "DEM2: D:\\Whiskeytown\\dem_diff\\brandy_creek\\dems\\orig\\brandy_dem2019.tif\n",
      "ErrorTrendRaster: D:\\Whiskeytown\\dem_diff\\brandy_creek\\demdiff19-18\\detrend\\Detrend_19-18_polyn3.tif\n",
      "\n",
      "DoD = DEM2 - DEM1\n",
      "\n",
      "Method1:\n",
      "AdjustedDoD = UnadjustedDoD - ErrorTrendRaster\n",
      "\n",
      "Method2 (should be identical to Method1): \n",
      "Co-RegisteredDEM1 = DEM1 + ErrorTrendRaster\n",
      "AltAdjustedDoD = DEM2 - Co-RegisteredDEM1\n",
      "\n",
      "Difference between Method1 and Method2 rasters (should be zero)\n",
      "MEAN = 0\n",
      "STD = 0\n",
      "MAXIMUM = 0\n",
      "MINIMUM = 0\n"
     ]
    }
   ],
   "source": [
    "#Sanity check to double check that DoD error correction is being done with teh right signs\n",
    "PolynomialOrderSurface = 3\n",
    "\n",
    "#inputs\n",
    "intrendrasterstem = outtrendrasterstem #from cell above\n",
    "trendraster = intrendrasterstem + str(PolynomialOrderSurface) + '.tif'\n",
    "print(f'DEM1: {dem1}\\n'\n",
    "      f'DEM2: {dem2}\\n'\n",
    "      f'ErrorTrendRaster: {trendraster}\\n\\n'\n",
    "      f'DoD = DEM2 - DEM1\\n')\n",
    "\n",
    "#Adjust DoD with trend raster\n",
    "print(f'Method1:\\n'\n",
    "      f'AdjustedDoD = UnadjustedDoD - ErrorTrendRaster\\n')\n",
    "unadjdod = RasterCalculator([str(dem1), str(dem2)], [\"dem1\", \"dem2\"],\n",
    "                                       \"dem2 - dem1\", \"FirstOf\", \"FirstOf\")\n",
    "adjdod = RasterCalculator([unadjdod, trendraster], [\"dod\", \"trendraster\"],\n",
    "                                       \"dod - trendraster\", \"FirstOf\", \"FirstOf\")\n",
    "#Alternatively, Co-Register DEM1 to DEM2 using ErrorTrendRaster, then derive adjusted DoD.  \n",
    "#Should be identical\n",
    "print(f'Method2 (should be identical to Method1): \\n'\n",
    "      f'Co-RegisteredDEM1 = DEM1 + ErrorTrendRaster\\n'\n",
    "      f'AltAdjustedDoD = DEM2 - Co-RegisteredDEM1\\n')\n",
    "coregdem1 = RasterCalculator([str(dem1), trendraster], [\"dem1\", \"trendraster\"],\n",
    "                                       \"dem1 + trendraster\", \"FirstOf\", \"FirstOf\")\n",
    "altadjdod = RasterCalculator([coregdem1, str(dem2)], [\"coregdem1\", \"dem2\"],\n",
    "                                       \"dem2 - coregdem1\", \"FirstOf\", \"FirstOf\")\n",
    "\n",
    "doddiff = RasterCalculator([adjdod, altadjdod], [\"adjdod\", \"altadjdod\"],\n",
    "                                       \"adjdod - altadjdod\", \"FirstOf\", \"FirstOf\")\n",
    "\n",
    "#write output to get statistics\n",
    "doddiff.save(arcpy.env.workspace + '\\DoDDiffSanityCheck.tif')\n",
    "print(f'Difference between Method1 and Method2 rasters (should be zero)')\n",
    "#Get stats of doddiff\n",
    "for stat in ['MEAN', 'STD', 'MAXIMUM', 'MINIMUM']:\n",
    "    doddiff_result = arcpy.management.GetRasterProperties(arcpy.env.workspace + '\\DoDDiffSanityCheck.tif', stat);\n",
    "    outstat = doddiff_result.getOutput(0);\n",
    "    print(f'{stat} = {str(outstat)}')\n",
    "\n",
    "#clean up\n",
    "arcpy.management.Delete(arcpy.env.workspace + '\\DoDDiffSanityCheck.tif');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contemporary-planner",
   "metadata": {},
   "source": [
    "### 6. Decide which trend surface to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "broadband-purple",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        dod_unadj    poly0resid    poly1resid     poly2resid     poly3resid  \\\n",
      "0   336555.000000  3.365550e+05  3.365550e+05  336555.000000  336555.000000   \n",
      "1       -0.183162  1.779189e-09  4.230353e-09      -0.000147      -0.000056   \n",
      "2        0.212388  2.123879e-01  1.908409e-01       0.181654       0.181654   \n",
      "3       -6.121185 -5.938023e+00 -5.933700e+00      -5.915316      -5.915233   \n",
      "4       -0.634526 -4.513637e-01 -3.619353e-01      -0.349134      -0.349045   \n",
      "5       -0.275787 -9.262498e-02 -9.024726e-02      -0.084849      -0.084756   \n",
      "6       -0.149628  3.353469e-02  1.134843e-03       0.006973       0.007062   \n",
      "7       -0.059570  1.235921e-01  9.501396e-02       0.090649       0.090741   \n",
      "8        0.137848  3.210103e-01  3.674790e-01       0.327802       0.327895   \n",
      "9        0.970703  1.153866e+00  1.291804e+00       1.191322       1.191415   \n",
      "10  264783.000000  2.647830e+05  2.647830e+05  264783.000000  264783.000000   \n",
      "11      -0.166343  1.681918e-02  4.446373e-03      -0.013242      -0.013151   \n",
      "12       0.221408  2.214079e-01  2.009906e-01       0.188935       0.188936   \n",
      "13      -6.121185 -5.938023e+00 -5.933700e+00      -5.915316      -5.915233   \n",
      "14      -0.649547 -4.663844e-01 -3.766230e-01      -0.379225      -0.379140   \n",
      "15      -0.249878 -6.671555e-02 -8.658700e-02      -0.093030      -0.092942   \n",
      "16      -0.130829  5.233352e-02  8.772761e-04      -0.005042      -0.004953   \n",
      "17      -0.040436  1.427266e-01  9.899843e-02       0.073240       0.073331   \n",
      "18       0.156752  3.399144e-01  3.895700e-01       0.328976       0.329067   \n",
      "19       0.970703  1.153866e+00  1.291804e+00       1.191322       1.191415   \n",
      "20   71769.000000  7.176900e+04  7.176900e+04   71769.000000   71769.000000   \n",
      "21      -0.245217 -6.205416e-02 -1.640598e-02       0.048162       0.048253   \n",
      "22       0.160563  1.605632e-01  1.463116e-01       0.141694       0.141694   \n",
      "23      -1.317902 -1.134739e+00 -1.031575e+00      -0.889166      -0.889076   \n",
      "24      -0.597382 -4.142192e-01 -3.243694e-01      -0.233869      -0.233777   \n",
      "25      -0.332611 -1.494487e-01 -1.065981e-01      -0.039253      -0.039158   \n",
      "26      -0.218262 -3.509934e-02  1.992345e-03       0.059543       0.059636   \n",
      "27      -0.133636  4.952590e-02  8.456942e-02       0.138474       0.138570   \n",
      "28       0.015619  1.987813e-01  2.384034e-01       0.325127       0.325218   \n",
      "29       0.543488  7.266499e-01  8.046503e-01       0.807449       0.807546   \n",
      "\n",
      "       poly4resid     type   stat  \n",
      "0   336555.000000      all  count  \n",
      "1       -0.002392      all   mean  \n",
      "2        0.179877      all    std  \n",
      "3       -5.890299      all    min  \n",
      "4       -0.347562      all   2.5%  \n",
      "5       -0.085581      all    25%  \n",
      "6        0.002561      all    50%  \n",
      "7        0.085257      all    75%  \n",
      "8        0.327497      all  97.5%  \n",
      "9        1.176968      all    max  \n",
      "10  264783.000000    paved  count  \n",
      "11      -0.011945    paved   mean  \n",
      "12       0.187314    paved    std  \n",
      "13      -5.890299    paved    min  \n",
      "14      -0.378395    paved   2.5%  \n",
      "15      -0.089928    paved    25%  \n",
      "16      -0.005229    paved    50%  \n",
      "17       0.073800    paved    75%  \n",
      "18       0.326956    paved  97.5%  \n",
      "19       1.176968    paved    max  \n",
      "20   71769.000000  unpaved  count  \n",
      "21       0.032852  unpaved   mean  \n",
      "22       0.143888  unpaved    std  \n",
      "23      -0.801617  unpaved    min  \n",
      "24      -0.230221  unpaved   2.5%  \n",
      "25      -0.063505  unpaved    25%  \n",
      "26       0.037356  unpaved    50%  \n",
      "27       0.122931  unpaved    75%  \n",
      "28       0.330244  unpaved  97.5%  \n",
      "29       0.861865  unpaved    max  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jlogan\\AppData\\Local\\ESRI\\conda\\envs\\arcgispro-py3-jlogan\\lib\\site-packages\\ipykernel_launcher.py:36: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n"
     ]
    }
   ],
   "source": [
    "#Extract residual values from each adjusted DoD trend surface and evaluate bulk points\n",
    "\n",
    "stablept_shp = str(Path(outdod_pt_dir, outunadjdod.stem + r'_stable_pts.shp')) #same as DoD, but '_pts.shp')\n",
    "inadjdodstem = str(Path(outdod_adj_dir, dodnamestem + '_' + aoi + r'_adj_polyn_'))\n",
    "outdetrendevalptshp = str(Path(outtrend_pt_dir, Path(outdod_pt_dir, outunadjdod.stem + r'_pts.shp') .stem + r'_detrend_eval.shp'))\n",
    "\n",
    "#copy stable points to eval file\n",
    "arcpy.Copy_management(stablept_shp, outdetrendevalptshp);\n",
    "\n",
    "#derive list of adjusted dods and output fieldnames for multi-raster extract\n",
    "inras_outfield_list = [[inadjdodstem + str(n) + '.tif', 'poly' + str(n) + 'resid'] for n in [0,1,2,3,4]]\n",
    "\n",
    "#extract adjusted dod residual values to points\n",
    "ExtractMultiValuesToPoints(outdetrendevalptshp, inras_outfield_list, \"NONE\")\n",
    "\n",
    "#read into gdf to evaluate\n",
    "tempgdf = gpd.read_file(outdetrendevalptshp)\n",
    "\n",
    "#evaluate paved/unpaved\n",
    "pavedgdfstat = tempgdf[tempgdf['type'] == 'paved'].drop(columns=['geometry','pointid', 'TARGET_FID','Join_Count','id']).describe([.025, .25, .5, .75, .975])\n",
    "pavedgdfstat['type'] = 'paved'\n",
    "pavedgdfstat['stat'] = pavedgdfstat.index\n",
    "unpavedgdfstat = tempgdf[tempgdf['type'] == 'unpaved'].drop(columns=['geometry','pointid', 'TARGET_FID','Join_Count','id']).describe([.025, .25, .5, .75, .975])\n",
    "unpavedgdfstat['type'] = 'unpaved'\n",
    "unpavedgdfstat['stat'] = unpavedgdfstat.index\n",
    "tempgdfstat = tempgdf.drop(columns=['geometry','pointid', 'TARGET_FID','Join_Count','id']).describe([.025, .25, .5, .75, .975])\n",
    "tempgdfstat['type'] = 'all'\n",
    "tempgdfstat['stat'] = tempgdfstat.index\n",
    "\n",
    "dfstat = pd.concat([tempgdfstat, pavedgdfstat, unpavedgdfstat], ignore_index=True)\n",
    "\n",
    "# Evaluate bulk residual values and write to file\n",
    "dfstat.to_csv(Path(outtrendraster_dir, detrendnamestem + r'bulk_pt_adjustment_eval.csv'))\n",
    "\n",
    "# Evaluate bulk residual values grouped by polygon and write to file\n",
    "tempgdf.groupby(['id'])['dod_unadj','poly0resid','poly1resid','poly2resid','poly3resid','poly4resid'].agg(['mean','std']).describe([.025, .25, .5, .75, .975]).to_csv(\n",
    "    Path(outtrendraster_dir, detrendnamestem + r'bulk_pt_adjustment_bypolygon_eval.csv'))\n",
    "\n",
    "print(dfstat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "abandoned-treat",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trend surface chosen\n",
    "\n",
    "#For Brandy Creek 2018-2019\n",
    "SelectedPolynomialOrderSurface = 3  #Based on visual inspection of trend surface and comparison of Adjusted DoDs\n",
    "\n",
    "# #For Boulder Creek 2018-2019\n",
    "# SelectedPolynomialOrderSurface = 3  #Based on visual inspection of trend surface and comparison of Adjusted DoDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personal-python",
   "metadata": {},
   "source": [
    "### 7. Make co-registered DEMs for archival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "furnished-illustration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEM to be coregistered, DEM1: D:\\Whiskeytown\\dem_diff\\brandy_creek\\dems\\orig\\brandy_dem2018.tif\n",
      "ErrorTrendRaster: D:\\Whiskeytown\\dem_diff\\brandy_creek\\demdiff19-18\\detrend\\Detrend_19-18_polyn3.tif\n",
      "\n",
      "Co-RegisteredDEM1 = DEM1 + ErrorTrendRaster\n",
      "\n",
      "brandy_dem2019.tif copied to D:\\Whiskeytown\\dem_diff\\brandy_creek\\demdiff19-18\\co-reg-dem\\brandy_dem2019.tif\n",
      "\n",
      "Co-registered brandy_dem2018.tif copied to D:\\Whiskeytown\\dem_diff\\brandy_creek\\demdiff19-18\\co-reg-dem\\brandy_dem2018_coreg_errortrendpolyn_3.tif\n"
     ]
    }
   ],
   "source": [
    "#Apply Error Trend Surface to DEM1 Create co-registered DEM for archiving\n",
    "PolynomialOrderSurface = SelectedPolynomialOrderSurface\n",
    "\n",
    "#inputs\n",
    "intrendrasterstem = outtrendrasterstem #from cell above\n",
    "trendraster = intrendrasterstem + str(PolynomialOrderSurface) + '.tif'\n",
    "print(f'DEM to be coregistered, DEM1: {dem1}\\n'\n",
    "      f'ErrorTrendRaster: {trendraster}\\n\\n'\n",
    "      f'Co-RegisteredDEM1 = DEM1 + ErrorTrendRaster\\n')\n",
    "\n",
    "#Copy DEM2 (unaltered) to directory\n",
    "arcpy.Copy_management(str(dem2), str(Path(outcoregdem_dir , str(dem2.name))));\n",
    "\n",
    "\n",
    "#Co-Register DEM1 to DEM2 using ErrorTrendRaster\n",
    "coregdem1 = RasterCalculator([str(dem1), trendraster], [\"dem1\", \"trendraster\"],\n",
    "                                       \"dem1 + trendraster\", \"FirstOf\", \"FirstOf\")\n",
    "\n",
    "#write output \n",
    "coregdem1.save(str(Path(outcoregdem_dir , str(dem1.stem) + '_coreg_errortrendpolyn_' + str(PolynomialOrderSurface) + '.tif')))\n",
    "print(f'{dem2.name} copied to {str(Path(outcoregdem_dir , str(dem2.name)))}\\n')\n",
    "outcoregpath = str(Path(outcoregdem_dir , str(dem1.stem) + '_coreg_errortrendpolyn_' + str(PolynomialOrderSurface) + '.tif'))\n",
    "print(f'Co-registered {dem1.name} copied to {outcoregpath}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "weighted-length",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subdividing polygons ...\n",
      "Joining sub-polygon id to stable points ...\n",
      "Subpolygons written to: \n",
      "     D:\\Whiskeytown\\dem_diff\\brandy_creek\\shp\\Stable_poly_18-19_expanded_500m.shp\n",
      "Stable points with sub-polygon id written to: \n",
      "     D:\\Whiskeytown\\dem_diff\\brandy_creek\\demdiff19-18\\dod\\shp\\DoD_19-18_brandy_unadj_stable_pts500m_subpolyid.shp\n"
     ]
    }
   ],
   "source": [
    "# Subdivide stable polygons to use as test/train sets for residual error evaluation, then get poly and subdiv poly id onto points via intersection\n",
    "\n",
    "#inputs\n",
    "# target_subpoly_area = 9 # (9 = 144 points)size of subpolygons in m^2 (too small may result in overfitting of trend surface to validation points?)\n",
    "# target_subpoly_area = 100 # (100 = 1600 pts) size of subpolygons in m^2(too small may result in overfitting of trend surface to validation points?\n",
    "target_subpoly_area = 500 #larger polygons to  avoid overfitting of trend surface to validation points?\n",
    "#points\n",
    "stablept_shp = str(Path(outdod_pt_dir, outunadjdod.stem + r'_stable_pts.shp')) #same as DoD, but '_pts.shp')\n",
    "outpointswithsubpolyid_str =  str(Path(outdod_pt_dir, outunadjdod.stem + r'_stable_pts' + str(target_subpoly_area) + r'm_subpolyid.shp'))\n",
    "\n",
    "#polygons\n",
    "\n",
    "stablepolyshp_str = str(stablepolyshp)\n",
    "outstablepolysubdivided_str = str(Path(str(stablepolyshp.with_suffix('')) + '_' + str(target_subpoly_area) + 'm.shp'))\n",
    "\n",
    "#subdivide polygon\n",
    "print('Subdividing polygons ...')\n",
    "arcpy.SubdividePolygon_management(\n",
    "    stablepolyshp_str, outstablepolysubdivided_str, \"EQUAL_AREAS\",\"\", target_subpoly_area, \"\", \"\", \n",
    "    \"STACKED_BLOCKS\");\n",
    "\n",
    "#add subdiv poly id to poly attr table\n",
    "polygdf = gpd.read_file(outstablepolysubdivided_str)\n",
    "polygdf['subpolyid'] = polygdf.index\n",
    "polygdf.to_file(outstablepolysubdivided_str)\n",
    "\n",
    "#spatial join stable points with subdiv polygons to get polyid and subdividedpolyid on points for later test/train filter.\n",
    "print('Joining sub-polygon id to stable points ...')\n",
    "ptgdf = gpd.read_file(stablept_shp)\n",
    "\n",
    "ptwithpolygdf = gpd.sjoin(ptgdf, polygdf, how=\"left\", op='intersects')\n",
    "ptwithpolygdf.drop(columns=['id_right', 'type_right','index_right'], inplace=True)\n",
    "ptwithpolygdf.rename(columns = {'id_left': 'id', 'type_left': 'type'}, inplace=True)\n",
    "\n",
    "#write output shp\n",
    "ptwithpolygdf.to_file(outpointswithsubpolyid_str)\n",
    "\n",
    "print(f'Subpolygons written to: \\n     {outstablepolysubdivided_str}')\n",
    "print(f'Stable points with sub-polygon id written to: \\n     {outpointswithsubpolyid_str}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "harmful-haiti",
   "metadata": {},
   "source": [
    "### Evaluate Residual Error by splitting original polygons into test/train subsets\n",
    "- Withhold one polygon at a time for test set\n",
    "- Use remaining polygons to generate trend surface\n",
    "- Evaluate using withheld test polygon\n",
    "- Iterate for each polygon, then generate stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "concrete-alexander",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021/03/02 13:23:42\n",
      "Beginning iteration 0 of 13 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 13:31:43\n",
      "Beginning iteration 1 of 13 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 13:38:57\n",
      "Beginning iteration 2 of 13 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 13:45:39\n",
      "Beginning iteration 3 of 13 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 13:52:21\n",
      "Beginning iteration 4 of 13 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 13:59:14\n",
      "Beginning iteration 5 of 13 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 14:05:54\n",
      "Beginning iteration 6 of 13 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 14:12:59\n",
      "Beginning iteration 7 of 13 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 14:24:17\n",
      "Beginning iteration 8 of 13 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 14:31:15\n",
      "Beginning iteration 9 of 13 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 14:39:06\n",
      "Beginning iteration 10 of 13 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 14:48:41\n",
      "Beginning iteration 11 of 13 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 14:56:44\n",
      "Beginning iteration 12 of 13 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 15:03:41\n"
     ]
    }
   ],
   "source": [
    "#Evaluate Residual Error by splitting original polygons into test/train subsets, withholding one polygon at a time for test.\n",
    "#Create new trend surface with train subset, evaluate residual offset with test subset and write output to csv\n",
    "\n",
    "#order of polynomial for trend surface\n",
    "PolynomialOrder = SelectedPolynomialOrderSurface\n",
    "\n",
    "#Iterate and do test/train splits to evaluate by subpolygon\n",
    "print(datetime.now().strftime(\"%Y/%d/%m %H:%M:%S\"))    \n",
    "\n",
    "#inputs\n",
    "#stable pooint shapefile with polygon \"id\" field for filtering\n",
    "stablept_shp = str(Path(outdod_pt_dir, outunadjdod.stem + r'_stable_pts.shp')) #same as DoD, but '_pts.shp')\n",
    "#stable polygons, not subdivided\n",
    "stablepoly_str = str(stablepolyshp) #from input cell above\n",
    "\n",
    "#load points with subpoly id into gdf to be filtered\n",
    "ptwithpolygdf = gpd.read_file(stablept_shp)\n",
    "\n",
    "#trend surface details\n",
    "zField = \"dod_unadj\"\n",
    "cellSize = 0.25\n",
    "regressionType = \"LINEAR\"\n",
    "\n",
    "#result df\n",
    "resultsdf = pd.DataFrame(columns = ['bulk_pt_mean','bulk_pt_std','all_poly_mean','all_poly_std','paved_poly_mean','paved_poly_std','unpaved_poly_mean','unpaved_poly_std'])\n",
    "# #percent of poly to use for train \n",
    "# trainfrac = 0.6\n",
    "\n",
    "#read stable poly\n",
    "stablepolygdf = gpd.read_file(stablepoly_str)\n",
    "\n",
    "\n",
    "for i in range(len(stablepolygdf)):\n",
    "    #clean up scratch dir first (leave results of final iteration in scratch to let user see them)\n",
    "    for fn in [r\"\\temp_trainpoly.shp\", r\"\\temp_valpoly.shp\", r\"\\temp_trainpoint.shp\", r\"\\temp_valpoint.shp\", r\"\\temp_valpoint_withdetrend.shp\",r\"\\temp_trendraster.tif\"]:\n",
    "        arcpy.management.Delete(arcpy.env.workspace + fn)\n",
    "    print(f'Beginning iteration {str(i)} of {str(len(stablepolygdf))} ...')\n",
    "    \n",
    "    #Move one polygon at a time into test data set\n",
    "    trainpolygdf = stablepolygdf.drop([i])\n",
    "    valpolygdf = stablepolygdf.iloc[i]\n",
    "    \n",
    "    #use polyid to select train/val points\n",
    "    trainptgdf = ptwithpolygdf[ptwithpolygdf['id'].isin(trainpolygdf['id'].tolist())]\n",
    "    valptgdf = ptwithpolygdf[ptwithpolygdf['id'] == valpolygdf['id']]\n",
    "    \n",
    "    #write to temp\n",
    "    trainptgdf.to_file(arcpy.env.workspace + r\"\\temp_trainpoint.shp\")\n",
    "    valptgdf.to_file(arcpy.env.workspace + r\"\\temp_valpoint.shp\")\n",
    "    \n",
    "    #create trend surface\n",
    "    # Execute Trend\n",
    "    print('Creating trend surface...')\n",
    "    outTrend = Trend(arcpy.env.workspace + r\"\\temp_trainpoint.shp\", zField, cellSize, \n",
    "                     PolynomialOrder, regressionType)\n",
    "    outTrend.save(arcpy.env.workspace + r\"\\temp_trendraster.tif\")\n",
    "    \n",
    "    #Sample raster on validate points\n",
    "    print('Sampling trend surface...')\n",
    "    ExtractValuesToPoints(arcpy.env.workspace + r\"\\temp_valpoint.shp\", \n",
    "                          arcpy.env.workspace + r\"\\temp_trendraster.tif\", \n",
    "                          arcpy.env.workspace + r\"\\temp_valpoint_withdetrend.shp\",\n",
    "                          \"NONE\", \"VALUE_ONLY\")\n",
    "    \n",
    "    #read into gdf to eval\n",
    "    print('Evaluating trend surface...')\n",
    "    evalgdf = gpd.read_file(arcpy.env.workspace + r\"\\temp_valpoint_withdetrend.shp\")\n",
    "    #apply to DoD value (dod_unadj - trendRasterValue)\n",
    "    evalgdf['resid'] = evalgdf['dod_unadj'] - evalgdf['RASTERVALU']\n",
    "    \n",
    "    #gather residuals grouped by polygons\n",
    "    data = {'bulk_pt_mean': [evalgdf['resid'].mean()],\n",
    "            'bulk_pt_std': [evalgdf['resid'].std()],\n",
    "            'all_poly_mean': [evalgdf.groupby('id')['resid'].mean().mean()],\n",
    "            'all_poly_std': [evalgdf.groupby('id')['resid'].mean().std()],\n",
    "            'paved_poly_mean': [evalgdf[evalgdf['type'] == 'paved'].groupby('id')['resid'].mean().mean()],\n",
    "            'paved_poly_std': [evalgdf[evalgdf['type'] == 'paved'].groupby('id')['resid'].mean().std()],\n",
    "            'unpaved_poly_mean': [evalgdf[evalgdf['type'] == 'unpaved'].groupby('id')['resid'].mean().mean()],\n",
    "            'unpaved_poly_std': [evalgdf[evalgdf['type'] == 'unpaved'].groupby('id')['resid'].mean().std()]\n",
    "           }\n",
    "    resultsdf = resultsdf.append(pd.DataFrame(data, \n",
    "                                              columns = ['bulk_pt_mean','bulk_pt_std','all_poly_mean','all_poly_std','paved_poly_mean','paved_poly_std','unpaved_poly_mean','unpaved_poly_std']), \n",
    "                                 ignore_index=True)\n",
    "    \n",
    "    \n",
    "    print(datetime.now().strftime(\"%Y/%d/%m %H:%M:%S\"))  \n",
    "    \n",
    "    #write intermediate output, overwrite to save progress\n",
    "    resultsdf.to_csv(Path(outtrendraster_dir, detrendnamestem + r'_order_' + str(PolynomialOrder) + \n",
    "                                  r'_test-train_eval_by_whole_polygon.csv'))\n",
    "    resultsdf.describe([.025, .25, .5, .75, .975]).to_csv(Path(outtrendraster_dir, detrendnamestem + r'_order_' + str(PolynomialOrder) + \n",
    "                                  r'_test-train_eval_by_whole_polygon_summary_stats.csv'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "professional-stack",
   "metadata": {},
   "source": [
    "### Evaluate Residual Error by splitting subdivided polygons into test/train subsets.\n",
    "- Withhold 40% of polygons for test set\n",
    "- Use remaining 60% of polygons to generate trend surface\n",
    "- Evaluate residual using withheld subpolygons\n",
    "- Iterate 100 times, then generate stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "wired-account",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021/02/02 21:52:42\n",
      "Beginning iteration 0 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/02/02 22:01:11\n",
      "Beginning iteration 1 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/02/02 22:09:58\n",
      "Beginning iteration 2 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/02/02 22:18:48\n",
      "Beginning iteration 3 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/02/02 22:27:21\n",
      "Beginning iteration 4 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/02/02 22:35:50\n",
      "Beginning iteration 5 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/02/02 22:44:37\n",
      "Beginning iteration 6 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/02/02 22:53:10\n",
      "Beginning iteration 7 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/02/02 23:01:27\n",
      "Beginning iteration 8 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/02/02 23:10:18\n",
      "Beginning iteration 9 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/02/02 23:18:56\n",
      "Beginning iteration 10 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/02/02 23:27:40\n",
      "Beginning iteration 11 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/02/02 23:36:23\n",
      "Beginning iteration 12 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/02/02 23:44:57\n",
      "Beginning iteration 13 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/02/02 23:53:28\n",
      "Beginning iteration 14 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 00:02:11\n",
      "Beginning iteration 15 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 00:10:52\n",
      "Beginning iteration 16 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 00:19:13\n",
      "Beginning iteration 17 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 00:27:45\n",
      "Beginning iteration 18 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 00:36:13\n",
      "Beginning iteration 19 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 00:44:45\n",
      "Beginning iteration 20 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 00:53:07\n",
      "Beginning iteration 21 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 01:01:50\n",
      "Beginning iteration 22 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 01:10:32\n",
      "Beginning iteration 23 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 01:19:08\n",
      "Beginning iteration 24 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 01:27:52\n",
      "Beginning iteration 25 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 01:36:29\n",
      "Beginning iteration 26 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 01:45:02\n",
      "Beginning iteration 27 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 01:53:51\n",
      "Beginning iteration 28 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 02:02:27\n",
      "Beginning iteration 29 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 02:11:06\n",
      "Beginning iteration 30 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 02:19:53\n",
      "Beginning iteration 31 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 02:28:31\n",
      "Beginning iteration 32 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 02:37:06\n",
      "Beginning iteration 33 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 02:45:35\n",
      "Beginning iteration 34 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 02:53:48\n",
      "Beginning iteration 35 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 03:01:43\n",
      "Beginning iteration 36 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 03:06:58\n",
      "Beginning iteration 37 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 03:12:03\n",
      "Beginning iteration 38 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 03:17:09\n",
      "Beginning iteration 39 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 03:22:43\n",
      "Beginning iteration 40 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 03:28:07\n",
      "Beginning iteration 41 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 03:33:41\n",
      "Beginning iteration 42 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 03:39:17\n",
      "Beginning iteration 43 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 03:45:01\n",
      "Beginning iteration 44 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 03:50:31\n",
      "Beginning iteration 45 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 03:56:08\n",
      "Beginning iteration 46 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 04:01:43\n",
      "Beginning iteration 47 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 04:06:56\n",
      "Beginning iteration 48 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 04:11:53\n",
      "Beginning iteration 49 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 04:16:58\n",
      "Beginning iteration 50 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 04:22:09\n",
      "Beginning iteration 51 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 04:27:15\n",
      "Beginning iteration 52 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 04:32:14\n",
      "Beginning iteration 53 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 04:37:21\n",
      "Beginning iteration 54 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 04:42:14\n",
      "Beginning iteration 55 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 04:47:14\n",
      "Beginning iteration 56 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 04:52:02\n",
      "Beginning iteration 57 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 04:56:53\n",
      "Beginning iteration 58 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 05:01:43\n",
      "Beginning iteration 59 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 05:06:54\n",
      "Beginning iteration 60 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 05:11:49\n",
      "Beginning iteration 61 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 05:17:00\n",
      "Beginning iteration 62 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 05:22:12\n",
      "Beginning iteration 63 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 05:28:43\n",
      "Beginning iteration 64 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 05:36:57\n",
      "Beginning iteration 65 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 05:45:31\n",
      "Beginning iteration 66 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 05:53:57\n",
      "Beginning iteration 67 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 06:02:33\n",
      "Beginning iteration 68 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 06:11:08\n",
      "Beginning iteration 69 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 06:19:36\n",
      "Beginning iteration 70 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 06:28:21\n",
      "Beginning iteration 71 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 06:37:08\n",
      "Beginning iteration 72 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 06:45:44\n",
      "Beginning iteration 73 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 06:54:13\n",
      "Beginning iteration 74 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 07:02:49\n",
      "Beginning iteration 75 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 07:11:21\n",
      "Beginning iteration 76 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 07:19:47\n",
      "Beginning iteration 77 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 07:28:12\n",
      "Beginning iteration 78 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 07:36:28\n",
      "Beginning iteration 79 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 07:44:46\n",
      "Beginning iteration 80 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 07:53:10\n",
      "Beginning iteration 81 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 08:02:00\n",
      "Beginning iteration 82 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 08:10:30\n",
      "Beginning iteration 83 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 08:19:02\n",
      "Beginning iteration 84 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 08:27:24\n",
      "Beginning iteration 85 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 08:36:00\n",
      "Beginning iteration 86 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 08:44:35\n",
      "Beginning iteration 87 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 08:53:11\n",
      "Beginning iteration 88 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 09:01:31\n",
      "Beginning iteration 89 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 09:10:13\n",
      "Beginning iteration 90 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 09:18:54\n",
      "Beginning iteration 91 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 09:27:29\n",
      "Beginning iteration 92 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 09:36:04\n",
      "Beginning iteration 93 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 09:44:47\n",
      "Beginning iteration 94 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 09:53:15\n",
      "Beginning iteration 95 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 10:01:43\n",
      "Beginning iteration 96 ...\n",
      "Creating trend surface...\n",
      "Sampling trend surface...\n",
      "Evaluating trend surface...\n",
      "2021/03/02 10:10:21\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'D:\\\\Whiskeytown\\\\dem_diff\\\\boulder_creek\\\\demdiff19-18\\\\detrend\\\\Detrend_19-18_polyn_order_3_adjustment_eval_by_500m_polygon_summary_stats.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-efb69b10b283>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    110\u001b[0m                                   r'_adjustment_eval_by_' + str(target_subpoly_area) + r'm_polygon.csv'))\n\u001b[0;32m    111\u001b[0m     resultsdf_subpoly.describe().to_csv(Path(outtrendraster_dir, detrendnamestem + r'_order_' + str(PolynomialOrder) + \n\u001b[1;32m--> 112\u001b[1;33m                                      r'_adjustment_eval_by_' + str(target_subpoly_area) + r'm_polygon_summary_stats.csv'))\n\u001b[0m\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[0mresultsdf_poly\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouttrendraster_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdetrendnamestem\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34mr'_order_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPolynomialOrder\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34mr'_adjustment_eval_by_whole_polygon.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\ESRI\\conda\\envs\\arcgispro-py3-jlogan\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors)\u001b[0m\n\u001b[0;32m   3168\u001b[0m             \u001b[0mdecimal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecimal\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3169\u001b[0m         )\n\u001b[1;32m-> 3170\u001b[1;33m         \u001b[0mformatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3172\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\ESRI\\conda\\envs\\arcgispro-py3-jlogan\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    188\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m                 \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompression_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m             )\n\u001b[0;32m    192\u001b[0m             \u001b[0mclose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\ESRI\\conda\\envs\\arcgispro-py3-jlogan\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors)\u001b[0m\n\u001b[0;32m    491\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m             \u001b[1;31m# No explicit encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'D:\\\\Whiskeytown\\\\dem_diff\\\\boulder_creek\\\\demdiff19-18\\\\detrend\\\\Detrend_19-18_polyn_order_3_adjustment_eval_by_500m_polygon_summary_stats.csv'"
     ]
    }
   ],
   "source": [
    "#Evaluate Residual Error by splitting subdivided polygons into test/train subsets.\n",
    "#Create new trend surface with train subset, evaluate residual offset with test subset\n",
    "#group evaluations by both \"subpoly\" and \"whole poly\", and write output to csv\n",
    "\n",
    "#order of polynomial for trend surface\n",
    "PolynomialOrder = SelectedPolynomialOrderSurface\n",
    "\n",
    "#Iterate and do test/train splits to evaluate by subpolygon\n",
    "print(datetime.now().strftime(\"%Y/%d/%m %H:%M:%S\"))    \n",
    "#number of iterations (~2-3 minutes each?)\n",
    "num_loops = 100\n",
    "\n",
    "#inputs\n",
    "pointswithsubpolyid = outpointswithsubpolyid_str #from cell above\n",
    "stablepolysubdivided = outstablepolysubdivided_str #from cell above\n",
    "\n",
    "#load points with subpoly id into gdf to be filtered\n",
    "ptwithpolygdf = gpd.read_file(pointswithsubpolyid)\n",
    "\n",
    "#trend surface details\n",
    "zField = \"dod_unadj\"\n",
    "cellSize = 0.25\n",
    "regressionType = \"LINEAR\"\n",
    "\n",
    "#result df\n",
    "resultsdf_subpoly = pd.DataFrame(columns = ['bulk_pt_mean','bulk_pt_std','all_poly_mean','all_poly_std',\n",
    "                                            'paved_poly_mean','paved_poly_std','unpaved_poly_mean','unpaved_poly_std'])\n",
    "resultsdf_poly = pd.DataFrame(columns = ['bulk_pt_mean','bulk_pt_std','all_poly_mean','all_poly_std',\n",
    "                                         'paved_poly_mean','paved_poly_std','unpaved_poly_mean','unpaved_poly_std'])\n",
    "#percent of poly to use for train \n",
    "trainfrac = 0.6\n",
    "\n",
    "for i in range(num_loops):\n",
    "    #clean up scratch dir first (leave results of final iteration in scratch to let user see them)\n",
    "    for fn in [r\"\\temp_trainpoly.shp\", r\"\\temp_valpoly.shp\", r\"\\temp_trainpoint.shp\", r\"\\temp_valpoint.shp\", \n",
    "               r\"\\temp_valpoint_withdetrend.shp\",r\"\\temp_trendraster.tif\"]:\n",
    "        arcpy.management.Delete(arcpy.env.workspace + fn)\n",
    "    print(f'Beginning iteration {i} ...')\n",
    "    #subset poly to test/train\n",
    "    arcpy.ga.SubsetFeatures(stablepolysubdivided, \n",
    "                            arcpy.env.workspace + r\"\\temp_trainpoly.shp\", \n",
    "                            arcpy.env.workspace + r\"\\temp_valpoly.shp\", 60, \"PERCENTAGE_OF_INPUT\")\n",
    "    #load each to get poly id\n",
    "    trainpolygdf = gpd.read_file(arcpy.env.workspace + r\"\\temp_trainpoly.shp\")\n",
    "    valpolygdf = gpd.read_file(arcpy.env.workspace + r\"\\temp_valpoly.shp\")\n",
    "    \n",
    "    #use subpolyid to select train/val points\n",
    "    trainptgdf = ptwithpolygdf[ptwithpolygdf['subpolyid'].isin(trainpolygdf['subpolyid'].tolist())]\n",
    "    valptgdf = ptwithpolygdf[ptwithpolygdf['subpolyid'].isin(valpolygdf['subpolyid'].tolist())]\n",
    "    \n",
    "    #write to temp\n",
    "    trainptgdf.to_file(arcpy.env.workspace + r\"\\temp_trainpoint.shp\")\n",
    "    valptgdf.to_file(arcpy.env.workspace + r\"\\temp_valpoint.shp\")\n",
    "    \n",
    "    #create trend surface\n",
    "    # Execute Trend\n",
    "    print('Creating trend surface...')\n",
    "    outTrend = Trend(arcpy.env.workspace + r\"\\temp_trainpoint.shp\", zField, cellSize, \n",
    "                     PolynomialOrder, regressionType)\n",
    "    outTrend.save(arcpy.env.workspace + r\"\\temp_trendraster.tif\")\n",
    "    \n",
    "    #Sample raster on validate points\n",
    "    print('Sampling trend surface...')\n",
    "    ExtractValuesToPoints(arcpy.env.workspace + r\"\\temp_valpoint.shp\", \n",
    "                          arcpy.env.workspace + r\"\\temp_trendraster.tif\", \n",
    "                          arcpy.env.workspace + r\"\\temp_valpoint_withdetrend.shp\",\n",
    "                          \"NONE\", \"VALUE_ONLY\")\n",
    "    \n",
    "    #read into gdf to eval\n",
    "    print('Evaluating trend surface...')\n",
    "    evalgdf = gpd.read_file(arcpy.env.workspace + r\"\\temp_valpoint_withdetrend.shp\")\n",
    "    #apply to DoD value (dod_unadj - trendRasterValue)\n",
    "    evalgdf['resid'] = evalgdf['dod_unadj'] - evalgdf['RASTERVALU']\n",
    "    \n",
    "    #gather residuals grouped by subpolygons\n",
    "    data_subpoly = {'bulk_pt_mean': [evalgdf['resid'].mean()],\n",
    "            'bulk_pt_std': [evalgdf['resid'].std()],\n",
    "            'all_poly_mean': [evalgdf.groupby('subpolyid')['resid'].mean().mean()],\n",
    "            'all_poly_std': [evalgdf.groupby('subpolyid')['resid'].mean().std()],\n",
    "            'paved_poly_mean': [evalgdf[evalgdf['type'] == 'paved'].groupby('subpolyid')['resid'].mean().mean()],\n",
    "            'paved_poly_std': [evalgdf[evalgdf['type'] == 'paved'].groupby('subpolyid')['resid'].mean().std()],\n",
    "            'unpaved_poly_mean': [evalgdf[evalgdf['type'] == 'unpaved'].groupby('subpolyid')['resid'].mean().mean()],\n",
    "            'unpaved_poly_std': [evalgdf[evalgdf['type'] == 'unpaved'].groupby('subpolyid')['resid'].mean().std()]\n",
    "           }\n",
    "    resultsdf_subpoly = resultsdf_subpoly.append(pd.DataFrame(data_subpoly, \n",
    "                                              columns = ['bulk_pt_mean','bulk_pt_std','all_poly_mean','all_poly_std',\n",
    "                                                         'paved_poly_mean','paved_poly_std','unpaved_poly_mean','unpaved_poly_std']), \n",
    "                                 ignore_index=True)\n",
    "    \n",
    "    #gather residuals grouped by entire polygons\n",
    "    data_poly = {'bulk_pt_mean': [evalgdf['resid'].mean()],\n",
    "            'bulk_pt_std': [evalgdf['resid'].std()],\n",
    "            'all_poly_mean': [evalgdf.groupby('id')['resid'].mean().mean()],\n",
    "            'all_poly_std': [evalgdf.groupby('id')['resid'].mean().std()],\n",
    "            'paved_poly_mean': [evalgdf[evalgdf['type'] == 'paved'].groupby('id')['resid'].mean().mean()],\n",
    "            'paved_poly_std': [evalgdf[evalgdf['type'] == 'paved'].groupby('id')['resid'].mean().std()],\n",
    "            'unpaved_poly_mean': [evalgdf[evalgdf['type'] == 'unpaved'].groupby('id')['resid'].mean().mean()],\n",
    "            'unpaved_poly_std': [evalgdf[evalgdf['type'] == 'unpaved'].groupby('id')['resid'].mean().std()]\n",
    "           }\n",
    "    resultsdf_poly = resultsdf_poly.append(pd.DataFrame(data_poly, \n",
    "                                              columns = ['bulk_pt_mean','bulk_pt_std','all_poly_mean','all_poly_std',\n",
    "                                                         'paved_poly_mean','paved_poly_std','unpaved_poly_mean','unpaved_poly_std']), \n",
    "                                 ignore_index=True)\n",
    "    \n",
    "    \n",
    "    print(datetime.now().strftime(\"%Y/%d/%m %H:%M:%S\"))   \n",
    "        \n",
    "    #write intermediate output, overwrite to save progress\n",
    "    resultsdf_subpoly.to_csv(Path(outtrendraster_dir, detrendnamestem + r'_order_' + str(PolynomialOrder) + \n",
    "                                  r'_adjustment_eval_by_' + str(target_subpoly_area) + r'm_polygon.csv'))\n",
    "    resultsdf_subpoly.describe([.025, .25, .5, .75, .975]).to_csv(Path(outtrendraster_dir, detrendnamestem + r'_order_' + str(PolynomialOrder) + \n",
    "                                     r'_adjustment_eval_by_' + str(target_subpoly_area) + r'm_polygon_summary_stats.csv'))\n",
    "    \n",
    "    resultsdf_poly.to_csv(Path(outtrendraster_dir, detrendnamestem + r'_order_' + str(PolynomialOrder) + r'_adjustment_eval_by_whole_polygon.csv'))\n",
    "    resultsdf_poly.describe([.025, .25, .5, .75, .975]).to_csv(Path(outtrendraster_dir, detrendnamestem + r'_order_' + str(PolynomialOrder) + \n",
    "                                     r'_adjustment_eval_by_whole_polygon_summary_stats.csv'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regular-alias",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
